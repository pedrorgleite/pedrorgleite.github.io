<!DOCTYPE html>
<!--[if IE 8 ]><html class="no-js oldie ie8" lang="en"> <![endif]-->
<!--[if IE 9 ]><html class="no-js oldie ie9" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!--><html class="no-js" lang="en"> <!--<![endif]-->
<head>

   <!--- basic page needs
   ================================================== -->
   <meta charset="utf-8">
	<title>Heart Disease Prediction</title>

   <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    
</style>
<script
src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
type="text/javascript"></script>
<!--[if lt IE 9]>
  <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
<![endif]-->
</head>
<body>
    
<header id="title-block-header">
<h1 class="title"><strong>Heart Disease Prediction: Machine Learning
Approach</strong><br /></h1>
<p>A Comprehensive Report on the Analysis and Prediction of Heart
    Disease</p>
</header>
<div class="titlepage">
<div class="center">
<p><img src="images/heart.png" style="width:40.0%" alt="image" /><br />
<strong>Prepared by:</strong><br />
Pedro Leite<br />
Student Number: 61981213<br />
University of British Columbia<br />
</p>
</div>
</div>
<h1 id="sec:introduction">Introduction</h1>
<p>Heart disease represents one of the most significant health
challenges of our time, accounting for a considerable number of
mortalities worldwide. Predicting heart disease with high accuracy
remains a priority in the medical field to facilitate early intervention
and treatment planning.</p>
<p>This project focuses on utilizing a machine learning approach to
predict heart disease based on clinical datasets. The model’s objective
is not only to achieve high predictive accuracy but also to provide
insights into the critical factors contributing to heart disease.</p>
<figure>
<img src="images/ScatterPlotMatrix.png" id="fig:scatterplot_matrix"
style="width:100.0%"
alt="Scatterplot matrix demonstrating the relationships between various clinical features." />
<figcaption aria-hidden="true">Scatterplot matrix demonstrating the
relationships between various clinical features.</figcaption>
</figure>
<p>The image above (Figure <a href="#fig:scatterplot_matrix"
data-reference-type="ref" data-reference="fig:scatterplot_matrix">1</a>)
illustrates a scatterplot matrix, a crucial exploratory tool in our
analysis. It provides a visual examination of the potential
relationships between different clinical variables considered in the
heart disease prediction model. The scatterplot matrix enables us to
observe patterns, detect outliers, and discover structural relationships
between the variables, which are essential for building a reliable
predictive model.</p>
<p>The dataset utilized in this study includes a range of clinical
features such as age, sex, type of chest pain, resting blood pressure,
cholesterol levels, fasting blood sugar, resting electrocardiogram
results, maximum heart rate, exercise-induced angina, ST depression, and
the slope of the peak exercise ST segment.</p>
<p>In the following sections, we will discuss the methodology applied to
preprocess the data, the selection and optimization of machine learning
algorithms, and the evaluation of model performance. The insights gained
from this study are expected to contribute to the broader effort of
applying artificial intelligence in the field of predictive health
analytics.</p>
<h1 id="data-preprocessing">Data Preprocessing</h1>
<p>Data preprocessing is a crucial step in the machine learning
pipeline. It involves transforming raw data into an understandable
format for machine learning models. The preprocessing steps for the
Heart disease Prediction dataset included handling categorical and
numerical variables, as detailed below.</p>
<h2 id="numerical-variable-standardization">Numerical Variable
Standardization</h2>
<p>The numerical variables in the dataset, such as Age, Resting Blood
Pressure, and Cholesterol levels, were standardized. Standardization
involves rescaling the features so that they have a mean of 0 and a
standard deviation of 1. This was accomplished using the
<code>StandardScaler</code> from the scikit-learn library.</p>
<div class="sourceCode" id="cb1" data-language="Python"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating a transformer for numerical data</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>numerical_transformer <span class="op">=</span> StandardScaler()</span></code></pre></div>
<p>Standardization is important for models that are sensitive to the
scale of input features, like Logistic Regression, Support Vector
Machines (SVM), K-Nearest Neighbors (KNN), Principal Component Analysis
(PCA) and Neural Networks. It ensures that each feature contributes
equally to the model, preventing features with larger ranges from
dominating the model’s behavior.</p>
<h2 id="categorical-variable-encoding">Categorical Variable
Encoding</h2>
<p>Categorical variables such as Sex, Chest Pain Type, and Resting ECG
results were encoded to numerical values. This is because most machine
learning algorithms require numerical input. Encoding was done using the
<code>OneHotEncoder</code> from scikit-learn, which converts categorical
variables into a form that could be provided to ML algorithms.</p>
<div class="sourceCode" id="cb2" data-language="Python"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OneHotEncoder</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating a transformer for categorical data</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>categorical_transformer <span class="op">=</span> OneHotEncoder(handle_unknown<span class="op">=</span><span class="st">&#39;ignore&#39;</span>)</span></code></pre></div>
<p>One-hot encoding creates new binary columns for each category of a
variable. This method was chosen over label encoding due to the nominal
nature of the categorical variables where no ordinal relationship
exists.</p>
<h2 id="dataset-splitting-and-transformation-application">Dataset
Splitting and Transformation Application</h2>
<p>The dataset was split into training and testing sets to evaluate the
model’s performance on unseen data. A typical split ratio of 80% for
training and 20% for testing was used. This splitting ensures that the
model is trained on a large portion of the data, while still having a
sufficient amount of data for testing its generalization capability.</p>
<div class="sourceCode" id="cb3" data-language="Python"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Splitting the dataset into training (80%) and testing (20%) sets</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span></code></pre></div>
<h2 id="handling-anomalies-in-cholesterol-values">Handling Anomalies in
Cholesterol Values</h2>
<p>During the initial data exploration phase, a significant number of
instances were identified where cholesterol values are recorded as zero.
Given that cholesterol is a crucial lipid present in all humans, these
zero values are likely misrecorded or missing data, rather than true
physiological readings.</p>
<p>The presence of these zero values in the cholesterol variable posed a
challenge for our predictive modeling, as they could introduce bias and
affect the performance of machine learning algorithms. To address this,
we conducted a comparative analysis: one stream of analysis included
these records, while the other excluded them.</p>
<p>This comparative approach allowed us to directly assess the impact of
these anomalous values on the performance of our models. After thorough
analysis and evaluation, it was found that models trained on the dataset
with zero cholesterol values removed yielded more accurate results. This
conclusion was drawn from running and comparing two separate
implementations: <code>heart_predictor_og.ipynb</code> (which includes
zero cholesterol values) and <code>heart_predictor_clean.ipynb</code>
(which excludes zero cholesterol values). The latter demonstrated higher
accuracy, precision, recall, and F1-score.</p>
<p>Based on these findings, we decided to proceed with the dataset
excluding records with zero cholesterol values for our predictive
modeling.</p>
<h1 id="logistic-regression-for-heart-disease-prediction">Logistic
Regression for Heart Disease Prediction</h1>
<p>Logistic Regression was chosen for its effectiveness in binary
classification tasks, particularly in medical diagnostics. As a
statistical model, it predicts the probability of occurrence of an event
by fitting data to a logistic function. It is particularly suitable for
this project due to its simplicity, interpretability, and efficiency in
predicting dichotomous outcomes, such as the presence or absence of
heart disease.</p>
<h2 id="implementation-of-logistic-regression">Implementation of
Logistic Regression</h2>
<p>The Logistic Regression model was implemented using Python’s
scikit-learn library. The model was then trained on the transformed
training data.</p>
<div class="sourceCode" id="cb4" data-language="Python"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating and training the Logistic Regression model</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>logistic_model <span class="op">=</span> LogisticRegression(max_iter<span class="op">=</span><span class="dv">1000</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>logistic_model.fit(X_train_transformed, y_train)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Making predictions on the test set</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> logistic_model.predict(X_test_transformed)</span></code></pre></div>
<p>Logistic Regression is fundamentally a statistical method for binary
classification. It estimates the probability that a given input point
belongs to a certain class. Mathematically, the probability <span
class="math inline">\(p\)</span> that a given observation belongs to
class 1 can be expressed as:</p>
<p><span class="math display">\[p = \frac{1}{1 + e^{-(\beta_0 + \beta_1
x_1 + \beta_2 x_2 + \ldots + \beta_n x_n)}}\]</span></p>
<p>where <span class="math inline">\(e\)</span> is the base of the
natural logarithm, <span class="math inline">\(x_1, x_2, \ldots,
x_n\)</span> are the feature values, and <span
class="math inline">\(\beta_0, \beta_1, \ldots, \beta_n\)</span> are the
coefficients determined during the model training. This function is
known as the logistic function or the sigmoid function. The output is a
value between 0 and 1, which represents the probability of the
observation being in class 1. If this probability is greater than a
certain threshold (commonly 0.5), the observation is classified into
class 1; otherwise, it is classified into class 0.</p>
<h3 id="implementation-in-scikit-learn">Implementation in
scikit-learn</h3>
<p>The Logistic Regression model for predicting heart disease was
implemented using Python’s scikit-learn library, a popular tool for
machine learning applications.</p>
<div class="sourceCode" id="cb5" data-language="Python"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating and training the Logistic Regression model</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>logistic_model <span class="op">=</span> LogisticRegression(max_iter<span class="op">=</span><span class="dv">1000</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>logistic_model.fit(X_train_transformed, y_train)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Making predictions on the test set</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> logistic_model.predict(X_test_transformed)</span></code></pre></div>
<p><strong>Understanding the Functions</strong></p>
<ul>
<li><p><strong><code>LogisticRegression</code></strong>: This class is
used to create a Logistic Regression model. The parameters like
<code>max_iter</code> and <code>random_state</code> control the training
process. <code>max_iter</code> specifies the maximum number of
iterations taken for the solvers to converge, while
<code>random_state</code> ensures reproducibility of the
results.</p></li>
<li><p><strong><code>fit</code></strong>: The <code>fit</code> method is
used to train the Logistic Regression model on the training data. It
seeks the optimal set of coefficients that best correlate the input
features with the target variable. The training process utilizes various
optimization algorithms based on the solver specified, including
Newton’s Method, Limited-memory Broyden–Fletcher–Goldfarb–Shanno
(LBFGS), Stochastic Average Gradient Descent (SAG), and SAGA, each
tailored to different dataset characteristics and requirements.</p></li>
<li><p><strong><code>predict</code></strong>: After the model is
trained, the <code>predict</code> method is used to make predictions on
new data. It uses the learned coefficients to calculate the
probabilities and classify the observations into different classes based
on a defined threshold, usually 0.5.</p></li>
</ul>
<p>This implementation highlights the simplicity and efficiency of using
scikit-learn for building and deploying machine learning models,
particularly for tasks like binary classification in medical
diagnosis.</p>
<h2 id="model-performance">Model Performance</h2>
<p>The performance of the machine learning models was evaluated using
several key metrics: accuracy, precision, recall, and the F1-score.
These metrics provide insights into different aspects of the model’s
ability to correctly classify instances.</p>
<h3 id="confusion-matrix-visualization">Confusion Matrix
Visualization</h3>
<p>The confusion matrix offers a visual representation of the model’s
classification accuracy.</p>
<figure>
<img src="images/confusion_matrix_after.png" style="width:60.0%"
alt="Confusion Matrix for the Logistic Regression Model after Data Cleaning" />
<figcaption aria-hidden="true">Confusion Matrix for the Logistic
Regression Model after Data Cleaning</figcaption>
</figure>
<p>The confusion matrix shows the number of true positives, true
negatives, false positives, and false negatives, crucial for
understanding the model’s performance, particularly in distinguishing
between patients with and without heart disease.</p>
<h3 id="accuracy">Accuracy</h3>
<p>Accuracy is the most intuitive performance measure and represents the
ratio of correctly predicted observations to the total observations. It
is calculated as follows:</p>
<p><span class="math display">\[\text{Accuracy} = \frac{\text{Number of
Correct Predictions}}{\text{Total Number of Predictions}}\]</span></p>
<p>The model achieved an accuracy of approximately 89.33%, as evaluated
on the test dataset. This high level of accuracy indicates the model’s
robustness in classifying the presence or absence of heart disease.</p>
<h3 id="precision-recall-and-f1-score">Precision, Recall, and
F1-Score</h3>
<p>Precision is the proportion of positive identifications that were
correct, while recall measures the proportion of actual positives that
were identified correctly. The F1-score is the harmonic mean of
precision and recall. These metrics are defined as:</p>
<p><span class="math display">\[\text{Precision} = \frac{\text{True
Positives}}{\text{True Positives} + \text{False Positives}}\]</span>
<span class="math display">\[\text{Recall} = \frac{\text{True
Positives}}{\text{True Positives} + \text{False Negatives}}\]</span>
<span class="math display">\[\text{F1-Score} = 2 \times
\frac{\text{Precision} \times \text{Recall}}{\text{Precision} +
\text{Recall}}\]</span></p>
<p>The results after data cleaning, which excluded zero cholesterol
values, are as follows:</p>
<table>
<caption>Performance Metrics for the Model after Data Cleaning</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Class</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">F1-Score</th>
<th style="text-align: center;">Support</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">0 (No Heart Disease)</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.930</td>
<td style="text-align: center;">0.892</td>
<td style="text-align: center;">71</td>
</tr>
<tr class="even">
<td style="text-align: left;">1 (With Heart Disease)</td>
<td style="text-align: center;">0.932</td>
<td style="text-align: center;">0.861</td>
<td style="text-align: center;">0.895</td>
<td style="text-align: center;">79</td>
</tr>
</tbody>
</table>
<p>These metrics collectively provide a comprehensive understanding of
the model’s performance, highlighting its strengths and areas for
improvement. The high values in precision, recall, and F1-score for
class 1 (With Heart Disease) suggest that the model is particularly
effective in identifying patients with heart disease, an essential
capability in medical diagnostics.</p>
<p>These metrics were computed using scikit-learn’s classification
report functions, which internally calculate these values based on the
confusion matrix of the model’s predictions against the actual
labels.</p>
<div class="sourceCode" id="cb6" data-language="Python"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, y_pred))</span></code></pre></div>
<p>The results indicate that the model performs well in differentiating
between patients with and without heart disease, making it a valuable
tool in medical diagnostics.</p>
<h3 id="roc-auc-curve">ROC-AUC Curve</h3>
<p>The Receiver Operating Characteristic (ROC) curve is a graphical plot
that illustrates the diagnostic ability of a binary classifier system as
its discrimination threshold is varied. The Area Under the Curve (AUC)
provides a single measure of the model’s performance across all possible
classification thresholds. The ROC curve is plotted with the True
Positive Rate (TPR, or Recall) against the False Positive Rate (FPR),
calculated as:</p>
<figure>
<img src="images/roc_curve_logreg.png" style="width:60.0%"
alt="ROC Curve for the Model after Data Cleaning" />
<figcaption aria-hidden="true">ROC Curve for the Model after Data
Cleaning</figcaption>
</figure>
<p>An AUC close to 1 indicates a high level of model performance, while
an AUC close to 0.5 suggests no discriminative power, akin to random
guessing. In this project, the ROC-AUC curve achieved an AUC of 0.96,
which signifies a very high capacity for correctly classifying patients.
This level of performance is especially critical in medical settings
where the stakes for accurate diagnosis are high.</p>
<h3 id="precision-recall-curve">Precision-Recall Curve</h3>
<p>The Precision-Recall Curve is a plot that demonstrates the trade-off
between precision (the true positive rate) and recall (the positive
predictive value) at various threshold settings. It is particularly
informative in scenarios where the positive class is of specific
importance, which is often the case in medical diagnosis scenarios such
as predicting heart disease.</p>
<figure>
<img src="images/precision_recall_curve_logreg.png" style="width:60.0%"
alt="Precision-Recall Curve for the Logistic Regression Model" />
<figcaption aria-hidden="true">Precision-Recall Curve for the Logistic
Regression Model</figcaption>
</figure>
<p>In this project, the area under the Precision-Recall Curve (AUC) is
0.96, indicating a robust performance of the model in classifying
patients with heart disease. This high value of AUC reflects not only
the model’s accuracy but also its reliability: when the model predicts
that a patient has heart disease, there is a high probability that the
patient indeed has the condition.</p>
<h1 id="optimization-and-hyperparameter-tuning">Optimization and
Hyperparameter Tuning</h1>
<p>Optimizing a machine learning model involves fine-tuning the
hyperparameters to enhance the model’s predictive performance. This
process is crucial as it can significantly impact the efficiency and
accuracy of the model. Each hyperparameter can influence the learning
process in different ways, and finding the optimal combination of these
parameters is key to developing robust models.</p>
<h2 id="grid-search">Grid Search</h2>
<p>Grid search is a commonly used technique for hyperparameter
optimization in machine learning. It involves exhaustively searching
through a manually specified subset of the hyperparameter space of a
learning algorithm. The grid search algorithm must be guided by some
performance metric, typically measured by cross-validation on the
training set.</p>
<p>One crucial hyperparameter for models like Logistic Regression is
<span class="math inline">\(C\)</span>, the inverse of the
regularization strength. Regularization is a technique used to prevent
overfitting by discouraging overly complex models in some way. The
parameter <span class="math inline">\(C\)</span> serves as a control
variable that retains the model’s simplicity, with smaller values of
<span class="math inline">\(C\)</span> indicating stronger
regularization.</p>
<h3 id="results-of-grid-search">Results of Grid Search</h3>
<p>The grid search was implemented using the following code snippet:</p>
<div class="sourceCode" id="cb7" data-language="Python"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define parameter grid</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {<span class="st">&#39;classifier__C&#39;</span>: [<span class="fl">0.001</span>, <span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>, <span class="dv">1000</span>]}</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize GridSearchCV</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>grid_search <span class="op">=</span> GridSearchCV(estimator<span class="op">=</span>pipeline, <span class="op">\</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>param_grid<span class="op">=</span>param_grid, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform grid search</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>grid_search.fit(X_train_transformed, y_train)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Retrieve the best parameter and mean test scores</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>best_param <span class="op">=</span> grid_search.best_params_</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>mean_test_scores <span class="op">=</span> grid_search.cv_results_[<span class="st">&#39;mean_test_score&#39;</span>]</span></code></pre></div>
<p>The best parameter found was <span class="math inline">\(C =
0.1\)</span>, and the mean test scores associated with each <span
class="math inline">\(C\)</span> value explored were as follows:</p>
<pre><code>Mean Test Scores: [0.8238, 0.8255, 0.8440, 0.8372, 0.8355, 0.8372, 0.8372]</code></pre>
<p>The accuracy of the best model on the test data was 0.8867,
highlighting the effectiveness of grid search optimization. The
following figure illustrates the improvement in model performance as
<span class="math inline">\(C\)</span> was varied:</p>
<figure>
<img src="images/grid_search_c_optimization.png" style="width:80.0%"
alt="Improvement of Grid Search Optimization of C" />
<figcaption aria-hidden="true">Improvement of Grid Search Optimization
of <span class="math inline">\(C\)</span></figcaption>
</figure>
<p>This figure shows the mean test score (accuracy) for different values
of <span class="math inline">\(C\)</span>, where we can observe that
<span class="math inline">\(C = 0.1\)</span> provides the highest mean
accuracy. Following the optimization process, the best model achieved an
accuracy of 88.67% with the following performance metrics:</p>
<table>
<caption>Performance Metrics for the Best Model from Grid
Search</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Class</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">F1-Score</th>
<th style="text-align: center;">Support</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">0 (No Heart Disease)</td>
<td style="text-align: center;">0.8553</td>
<td style="text-align: center;">0.9155</td>
<td style="text-align: center;">0.8844</td>
<td style="text-align: center;">71</td>
</tr>
<tr class="even">
<td style="text-align: left;">1 (With Heart Disease)</td>
<td style="text-align: center;">0.9189</td>
<td style="text-align: center;">0.8608</td>
<td style="text-align: center;">0.8889</td>
<td style="text-align: center;">79</td>
</tr>
</tbody>
</table>
<p>This comprehensive evaluation demonstrates the utility of grid search
in hyperparameter tuning to enhance the predictive performance of
machine learning models.</p>
<h2 id="bayesian-search">Bayesian Search</h2>
<p>Bayesian Optimization is a strategy for the optimization of objective
functions that are expensive to evaluate. It is particularly useful when
the optimization problem is high-dimensional and lacks an analytical
expression. Bayesian Optimization relies on the Bayesian technique of
setting a prior over the objective function and combining it with
evidence to get a posterior function.</p>
<p>Bayesian Search is an application of Bayesian Optimization for
hyperparameter tuning in machine learning models. Unlike grid search,
which exhaustively searches through a predefined subset of the
hyperparameter space, Bayesian Search uses a probabilistic model to
guide the search in order to find the most promising hyperparameters to
evaluate in the true objective function.</p>
<p>The advantage of Bayesian Search lies in its efficiency. By
constructing a probabilistic model, it can prioritize the evaluation of
hyperparameters that are more likely to yield better performance. This
approach significantly reduces the number of function evaluations needed
to find optimal hyperparameters, making it particularly advantageous
when the function evaluations are computationally expensive.</p>
<h3 id="bayesian-search-mechanism">Bayesian Search Mechanism</h3>
<p>Bayesian Search is an advanced hyperparameter optimization technique
that operates under the principle of probability to pinpoint the most
effective hyperparameters for a given model. Its methodology contrasts
sharply with grid search, which exhaustively combs through a pre-defined
hyperparameter space. Instead, Bayesian Search constructs a
probabilistic model that maps hyperparameters to the probability of a
score on the objective function.</p>
<p>The process is as follows:</p>
<ol>
<li><p>Initiate the search with a random selection of hyperparameters to
construct an initial model of the objective function.</p></li>
<li><p>Utilize the outcomes to refine the probabilistic model,
discerning which hyperparameters are likely to yield improved
results.</p></li>
<li><p>Strategically navigate the hyperparameter space by balancing
exploration of new regions against the exploitation of known promising
areas.</p></li>
<li><p>With each iteration, Bayesian Search refines its search,
effectively zeroing in on the most promising hyperparameters.</p></li>
</ol>
<p>This intelligent search strategy is far more resource-efficient than
the exhaustive approach of grid search, especially beneficial for models
with large parameter spaces and datasets.</p>
<h4 id="cross-validation-integration">Cross-Validation Integration</h4>
<p>To ensure the robustness of the model evaluation, cross-validation is
integrated into the Bayesian Search process. It involves the following
steps:</p>
<ol>
<li><p>The dataset is partitioned into <span
class="math inline">\(k\)</span> subsets or folds.</p></li>
<li><p>The model is trained on <span class="math inline">\(k-1\)</span>
folds and tested on the remaining fold.</p></li>
<li><p>This process is repeated <span class="math inline">\(k\)</span>
times, with each fold being used exactly once as the test set.</p></li>
<li><p>The cross-validation score is computed as the average of the
<span class="math inline">\(k\)</span> evaluation scores obtained from
the testing on each fold.</p></li>
</ol>
<p>The integration of cross-validation provides a more reliable
assessment of the model’s performance, as it is evaluated against
multiple data subsets, reducing the risk of overfitting and ensuring
that the hyperparameters generalize well to unseen data.</p>
<div class="sourceCode" id="cb9" data-language="Python"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>search_space <span class="op">=</span> {<span class="st">&#39;classifier__C&#39;</span>: Real(<span class="fl">1e-6</span>, <span class="fl">1e+6</span>, prior<span class="op">=</span><span class="st">&#39;log-uniform&#39;</span>)}</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>bayes_search <span class="op">=</span> BayesSearchCV(</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>logreg_pipeline,</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    search_spaces<span class="op">=</span>search_space,</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    n_iter<span class="op">=</span><span class="dv">32</span>,  <span class="co"># Number of iterations</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span><span class="dv">5</span>,       <span class="co"># 5-fold cross-validation</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>,  <span class="co"># Use all available cores</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Fitting the model</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>bayes_search.fit(X_train, y_train)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Best model evaluation</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>best_model_bayes <span class="op">=</span> bayes_search.best_estimator_</span></code></pre></div>
<h3 id="results-from-bayesian-search">Results from Bayesian Search</h3>
<p>Bayesian Search optimization was employed to fine-tune the
hyperparameters of our logistic regression model. The optimal value
found for the hyperparameter <span class="math inline">\(C\)</span> is
as follows:</p>
<p><span class="math display">\[\text{Optimal \( C \)} =
0.0834\]</span></p>
<p>This optimized parameter led to a model with an accuracy of 89.33%,
which is a significant improvement over the baseline model. Below is a
detailed breakdown of the model’s performance metrics:</p>
<div id="tab:bayes_optimization_metrics">
<table>
<caption>Performance Metrics for Best Model after Bayesian
Optimization</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Class</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">F1-Score</th>
<th style="text-align: center;">Support</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">0 (No Heart Disease)</td>
<td style="text-align: center;">0.866667</td>
<td style="text-align: center;">0.915493</td>
<td style="text-align: center;">0.890411</td>
<td style="text-align: center;">71</td>
</tr>
<tr class="even">
<td style="text-align: left;">1 (With Heart Disease)</td>
<td style="text-align: center;">0.920000</td>
<td style="text-align: center;">0.873418</td>
<td style="text-align: center;">0.896104</td>
<td style="text-align: center;">79</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Accuracy</td>
<td colspan="4" style="text-align: center;">0.8933333333333333</td>
</tr>
<tr class="even">
<td style="text-align: left;">Macro Avg</td>
<td style="text-align: center;">0.893333</td>
<td style="text-align: center;">0.894455</td>
<td style="text-align: center;">0.893257</td>
<td style="text-align: center;">150</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Weighted Avg</td>
<td style="text-align: center;">0.894756</td>
<td style="text-align: center;">0.893333</td>
<td style="text-align: center;">0.893409</td>
<td style="text-align: center;">150</td>
</tr>
</tbody>
</table>
</div>
<p>These results showcase the efficacy of Bayesian Search in optimizing
model parameters, thereby enhancing the model’s predictive performance
and reliability.</p>
<h4 id="confusion-matrix">Confusion Matrix</h4>
<p>The confusion matrix is a crucial tool for visualizing the
performance of a classification algorithm. The matrix for our model is
shown below:</p>
<figure>
<img src="images/confusion_matrix_lg_bs.png" style="width:50.0%"
alt="Confusion Matrix for the model after Bayesian Optimization." />
<figcaption aria-hidden="true">Confusion Matrix for the model after
Bayesian Optimization.</figcaption>
</figure>
<p>It displays the numbers of true positive, true negative, false
positive, and false negative predictions, providing insight into the
precision and recall of the model.</p>
<h4 id="roc-curve">ROC Curve</h4>
<p>The Receiver Operating Characteristic (ROC) curve.</p>
<figure>
<img src="images/roc_curve_bs_lr.png" style="width:60.0%"
alt="ROC Curve indicating the model’s discriminative performance." />
<figcaption aria-hidden="true">ROC Curve indicating the model’s
discriminative performance.</figcaption>
</figure>
<p>The AUC value of 0.96 for the ROC curve implies a high level of
accuracy in the model’s classification ability.</p>
<h4 id="precision-recall-curve-1">Precision-Recall Curve</h4>
<p>The Precision-Recall Curve highlights.</p>
<figure>
<img src="images/precision_recall_curve_bs_lr.png" style="width:60.0%"
alt="Precision-Recall Curve showing the balance between precision and recall." />
<figcaption aria-hidden="true">Precision-Recall Curve showing the
balance between precision and recall.</figcaption>
</figure>
<p>Similarly, the Precision-Recall Curve achieves an AUC of 0.96,
confirming the model’s effectiveness in classifying the positive
class.</p>
<p>These visualizations collectively affirm the robustness of the model
post Bayesian Search optimization.</p>
<h3 id="bayesian-search-over-grid-search">Bayesian Search Over Grid
Search</h3>
<p>Based on our experience and the results obtained, Bayesian Search has
shown to be superior to grid search in terms of both efficiency and
performance. Grid search’s exhaustive nature often makes it impractical
for larger datasets and more complex models. On the other hand, Bayesian
Search efficiently navigates the search space and converges to optimal
hyperparameters faster.</p>
<p>Henceforth, we will employ Bayesian Search for hyperparameter tuning
across our models to ensure we find the best model in a computationally
efficient manner. This decision is supported by the improved accuracy
and performance metrics observed in our experiments with Bayesian
Optimization.</p>
<h1 id="support-vector-machine-svm">Support Vector Machine (SVM)</h1>
<h2 id="introduction-to-svm">Introduction to SVM</h2>
<p>Support Vector Machines (SVM) are a set of supervised learning
methods used for classification, regression, and outliers detection. The
SVM algorithm seeks to find the hyperplane that best separates the
classes by maximizing the margin between the closest points of the
classes, which are called support vectors.</p>
<h2 id="implementation">Implementation</h2>
<p>The following code snippet illustrates the implementation of the
above-described process:</p>
<div class="sourceCode" id="cb10" data-language="Python"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the parameter space</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>param_space <span class="op">=</span> {</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;classifier__C&#39;</span>: Real(<span class="fl">1e-6</span>, <span class="fl">1e+6</span>, prior<span class="op">=</span><span class="st">&#39;log-uniform&#39;</span>),</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;classifier__gamma&#39;</span>: Real(<span class="fl">1e-6</span>, <span class="fl">1e+1</span>, prior<span class="op">=</span><span class="st">&#39;log-uniform&#39;</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup the pipeline</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>pipeline_svm <span class="op">=</span> Pipeline([</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    (<span class="st">&#39;preprocessor&#39;</span>, preprocessor),</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    (<span class="st">&#39;classifier&#39;</span>, SVC(random_state<span class="op">=</span><span class="dv">0</span>, probability<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Configure and run Bayesian optimization</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>bayes_search_svm <span class="op">=</span> BayesSearchCV(</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>pipeline_svm,</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    search_spaces<span class="op">=</span>param_space,</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    n_iter<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>bayes_search_svm.fit(X_train, y_train)</span></code></pre></div>
<p>In the given code snippet, the <code>BayesSearchCV</code> function
from the <code>skopt</code> package is employed to conduct
hyperparameter optimization for an SVM classifier within a machine
learning pipeline. The parameter space for optimization is defined using
a log-uniform prior, which is well-suited for hyperparameters that have
an exponential impact on model performance. The <code>Pipeline</code>
function from Scikit-learn is utilized to sequentially apply a
predefined list of transforms along with a final estimator, in this
case, the SVM classifier. The pipeline ensures a smooth workflow where
data preprocessing steps and model training are applied in a cohesive
manner. The <code>BayesSearchCV</code> object is then instantiated,
specifying the estimator as the defined pipeline, the hyperparameter
space, the number of iterations for the search, the cross-validation
strategy, and the number of jobs for parallel processing. Finally, the
<code>fit</code> method is called on the <code>bayes_search_svm</code>
object, which initiates the optimization process and trains the SVM
model on the provided training data.</p>
<h2 id="results-and-discussion">Results and Discussion</h2>
<p>After tuning the hyperparameters using Bayesian search, the best
parameters for the SVM model were determined to be <span
class="math inline">\(C \approx 2.19\)</span> and <span
class="math inline">\(\gamma \approx 0.020\)</span>. With these
optimized hyperparameters, the SVM model achieved a test score accuracy
of approximately 0.8933.</p>
<figure>
<img src="images/roc_curve_svm.png" id="fig:pr_svm"
alt="Precision-Recall Curve for SVM" />
<figcaption aria-hidden="true">Precision-Recall Curve for
SVM</figcaption>
</figure>
<figure>
<img src="images/pr_curve_svm.png" id="fig:pr_svm"
alt="Precision-Recall Curve for SVM" />
<figcaption aria-hidden="true">Precision-Recall Curve for
SVM</figcaption>
</figure>
<p>The ROC and Precision-Recall curves indicate that the SVM classifier
performs well, with high area under the curve (AUC) scores of 0.96 for
both curves, showcasing the model’s strong discriminative power.</p>
<p>The detailed performance metrics for the model are presented
below:</p>
<div id="tab:svm_metrics">
<table>
<caption>Performance Metrics for SVM Model</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Class</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">F1-Score</th>
<th style="text-align: center;">Support</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">0 (No Heart Disease)</td>
<td style="text-align: center;">0.866667</td>
<td style="text-align: center;">0.915493</td>
<td style="text-align: center;">0.890411</td>
<td style="text-align: center;">71</td>
</tr>
<tr class="even">
<td style="text-align: left;">1 (With Heart Disease)</td>
<td style="text-align: center;">0.920000</td>
<td style="text-align: center;">0.873418</td>
<td style="text-align: center;">0.896104</td>
<td style="text-align: center;">79</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Accuracy</td>
<td colspan="4" style="text-align: center;">0.8933333333333333</td>
</tr>
<tr class="even">
<td style="text-align: left;">Macro Avg</td>
<td style="text-align: center;">0.893333</td>
<td style="text-align: center;">0.894455</td>
<td style="text-align: center;">0.893257</td>
<td style="text-align: center;">150</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Weighted Avg</td>
<td style="text-align: center;">0.894756</td>
<td style="text-align: center;">0.893333</td>
<td style="text-align: center;">0.893409</td>
<td style="text-align: center;">150</td>
</tr>
</tbody>
</table>
</div>
<p>These results indicate a high level of performance, with particularly
strong precision and recall for identifying patients with heart disease.
This reinforces the capability of the SVM model as an effective tool for
medical diagnostic support.</p>
<h1 id="decision-tree-model">Decision Tree Model</h1>
<p>Decision Trees are a non-parametric supervised learning method used
for classification and regression. The goal is to create a model that
predicts the value of a target variable by learning simple decision
rules inferred from the data features.</p>
<h2 id="implementation-1">Implementation</h2>
<p>The Decision Tree model is implemented using the Bayesian
optimization technique to fine-tune its hyperparameters. This process
involves defining a search space for parameters like maximum depth,
minimum samples split, and minimum samples leaf. Then, a Bayesian
optimization search is conducted over this space using cross-validation
to find the best model.</p>
<div class="sourceCode" id="cb11" data-language="Python"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the parameter space for the Bayesian search</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>param_space_dt <span class="op">=</span> {</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;classifier__max_depth&#39;</span>: Integer(<span class="dv">1</span>, <span class="dv">50</span>),</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;classifier__min_samples_split&#39;</span>: Real(<span class="fl">0.01</span>, <span class="fl">1.0</span>, prior<span class="op">=</span><span class="st">&#39;uniform&#39;</span>),</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;classifier__min_samples_leaf&#39;</span>: Integer(<span class="dv">1</span>, <span class="dv">50</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Update the pipeline to use a Decision Tree Classifier</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>pipeline_dt <span class="op">=</span> Pipeline(steps<span class="op">=</span>[</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    (<span class="st">&#39;preprocessor&#39;</span>, preprocessor),</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    (<span class="st">&#39;classifier&#39;</span>, DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Configure BayesSearchCV for the Decision Tree</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>bayes_search_dt <span class="op">=</span> BayesSearchCV(</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>pipeline_dt,</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    search_spaces<span class="op">=</span>param_space_dt,</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    n_iter<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Fitting the model</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>np.<span class="bu">int</span> <span class="op">=</span> <span class="bu">int</span></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>bayes_search_dt.fit(X_train, y_train)</span></code></pre></div>
<p>In this code, <code>BayesSearchCV</code> is employed to optimize the
Decision Tree Classifier. This optimization tool utilizes Bayesian
methods to search for the best hyperparameters within the predefined
space. The search involves trying 32 different sets of hyperparameters,
using 5-fold cross-validation to evaluate the performance of each set.
The <code>Pipeline</code> integrates the preprocessing steps and the
classifier, ensuring that the steps are applied sequentially and
efficiently. The use of <code>n_jobs=-1</code> allows the process to use
all available computing resources, enhancing the speed of the search. By
fitting the model with
<code>bayes_search_dt.fit(X_train, y_train)</code>, the best
hyperparameters are determined and the Decision Tree Classifier is
optimized accordingly.</p>
<h2 id="results-and-discussion-1">Results and Discussion</h2>
<p>After applying Bayesian optimization, the Decision Tree model’s best
parameters were identified. The model demonstrated a good fit to the
test data, as evidenced by the accuracy and the classification report.
The results indicated the model’s effectiveness in differentiating
between the classes.</p>
<figure>
<img src="images/roc_curve_dt.png" id="fig:pr_dt"
alt="Precision-Recall Curve" />
<figcaption aria-hidden="true">Precision-Recall Curve</figcaption>
</figure>
<figure>
<img src="images/pr_curve_dt.png" id="fig:pr_dt"
alt="Precision-Recall Curve" />
<figcaption aria-hidden="true">Precision-Recall Curve</figcaption>
</figure>
<div id="tab:classification_report_dt">
<table>
<caption>Classification Report for Decision Tree Model</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Class</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">F1-Score</th>
<th style="text-align: center;">Support</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">0 (No Heart Disease)</td>
<td style="text-align: center;">0.764706</td>
<td style="text-align: center;">0.915493</td>
<td style="text-align: center;">0.833333</td>
<td style="text-align: center;">71</td>
</tr>
<tr class="even">
<td style="text-align: left;">1 (With Heart Disease)</td>
<td style="text-align: center;">0.907692</td>
<td style="text-align: center;">0.746835</td>
<td style="text-align: center;">0.819444</td>
<td style="text-align: center;">79</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Accuracy</td>
<td colspan="4" style="text-align: center;">0.8266666666666667</td>
</tr>
<tr class="even">
<td style="text-align: left;">Macro Avg</td>
<td style="text-align: center;">0.836199</td>
<td style="text-align: center;">0.831164</td>
<td style="text-align: center;">0.826389</td>
<td style="text-align: center;">150</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Weighted Avg</td>
<td style="text-align: center;">0.840012</td>
<td style="text-align: center;">0.826667</td>
<td style="text-align: center;">0.826019</td>
<td style="text-align: center;">150</td>
</tr>
</tbody>
</table>
</div>
<p>The ROC and Precision-Recall curves indicate that the SVM classifier
performs well, with high area under the curve (AUC) scores of 0.92 for
both curves. These results showcased this model’s weaker discriminative
power than SVM and Logistic regression models.</p>
<p>The Decision Tree classifier, tuned using Bayesian optimization,
showed promising results. However, the model’s accuracy suggests that
there is room for improvement, possibly by further hyperparameter tuning
or using more advanced models like Random Forest.</p>
<h1 id="random-forest-model">Random Forest Model</h1>
<p>Random Forests are an ensemble learning method that operates by
constructing multiple decision trees during training and outputting the
class that is the mode of the classes or mean prediction of the
individual trees. This method is known for its high accuracy,
robustness, and ease of use.</p>
<h2 id="implementation-2">Implementation</h2>
<p>Random Forests are implemented using Bayesian optimization for
hyperparameter tuning. The key hyperparameters include the number of
trees in the forest, the maximum depth of the trees, minimum samples
split, and minimum samples leaf. The Bayesian optimization technique
helps in efficiently finding the optimal values for these
parameters.</p>
<div class="sourceCode" id="cb12" data-language="Python"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the parameter space for the Bayesian search</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>param_space_rf <span class="op">=</span> {</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;classifier__max_depth&#39;</span>: Integer(<span class="dv">1</span>, <span class="dv">100</span>),</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;classifier__min_samples_split&#39;</span>: Real(<span class="fl">0.01</span>, <span class="fl">1.0</span>, prior<span class="op">=</span><span class="st">&#39;uniform&#39;</span>),</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;classifier__min_samples_leaf&#39;</span>: Integer(<span class="dv">1</span>, <span class="dv">50</span>),</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;classifier__n_estimators&#39;</span>: Integer(<span class="dv">10</span>, <span class="dv">1000</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Update the pipeline to use a Random Forest Classifier</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>pipeline_rf <span class="op">=</span> Pipeline(steps<span class="op">=</span>[</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    (<span class="st">&#39;preprocessor&#39;</span>, preprocessor),</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    (<span class="st">&#39;classifier&#39;</span>, RandomForestClassifier(random_state<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Configure BayesSearchCV for the Random Forest</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>bayes_search_rf <span class="op">=</span> BayesSearchCV(</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>pipeline_rf,</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    search_spaces<span class="op">=</span>param_space_rf,</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    n_iter<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Fitting the model</span></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>bayes_search_rf.fit(X_train, y_train)</span></code></pre></div>
<h2 id="results-and-discussion-2">Results and Discussion</h2>
<p>After applying Bayesian optimization to the Random Forest model, the
best parameters were identified. The model showed excellent performance
on the test data, as demonstrated by its high accuracy and balanced
precision-recall scores.</p>
<figure>
<img src="images/roc_curve_rf.png" id="fig:pr_rf"
alt="Precision-Recall Curve " />
<figcaption aria-hidden="true">Precision-Recall Curve </figcaption>
</figure>
<figure>
<img src="images/pr_curve_rf.png" id="fig:pr_rf"
alt="Precision-Recall Curve " />
<figcaption aria-hidden="true">Precision-Recall Curve </figcaption>
</figure>
<div id="tab:classification_report_rf">
<table>
<caption>Classification Report for Random Forest Model</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Class</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">F1-Score</th>
<th style="text-align: center;">Support</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">0 (No Heart Disease)</td>
<td style="text-align: center;">0.927536</td>
<td style="text-align: center;">0.901408</td>
<td style="text-align: center;">0.914286</td>
<td style="text-align: center;">71</td>
</tr>
<tr class="even">
<td style="text-align: left;">1 (With Heart Disease)</td>
<td style="text-align: center;">0.913580</td>
<td style="text-align: center;">0.936709</td>
<td style="text-align: center;">0.925000</td>
<td style="text-align: center;">79</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Accuracy</td>
<td colspan="4" style="text-align: center;">0.920000</td>
</tr>
<tr class="even">
<td style="text-align: left;">Macro Avg</td>
<td style="text-align: center;">0.920558</td>
<td style="text-align: center;">0.919059</td>
<td style="text-align: center;">0.919643</td>
<td style="text-align: center;">150</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Weighted Avg</td>
<td style="text-align: center;">0.920186</td>
<td style="text-align: center;">0.920000</td>
<td style="text-align: center;">0.919929</td>
<td style="text-align: center;">150</td>
</tr>
</tbody>
</table>
</div>
<p>The Random Forest classifier, with ROC AUC of 0.95 and PR AUC of
0.95, showcases its effectiveness in classification, particularly in
handling complex datasets with high accuracy.</p>
<h1 id="k-nearest-neighbors-knn-model">K-Nearest Neighbors (KNN)
Model</h1>
<p>The K-Nearest Neighbors algorithm is a simple, yet effective machine
learning method used for both classification and regression. It
classifies a data point based on how its neighbors are classified. The
algorithm identifies the ’k’ nearest neighbors to a data point and then
classifies it based on the majority vote of these neighbors.</p>
<h2 id="implementation-3">Implementation</h2>
<p>KNN is implemented with Bayesian optimization for hyperparameter
tuning. Key hyperparameters include the number of neighbors
(<code>n_neighbors</code>), the power parameter for the Minkowski metric
(<code>p</code>), and the weight function used in prediction
(<code>weights</code>). Bayesian optimization is employed to efficiently
find the optimal values for these parameters.</p>
<div class="sourceCode" id="cb13" data-language="Python"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the parameter space for the Bayesian search</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>param_space_knn <span class="op">=</span> {</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;classifier__n_neighbors&#39;</span>: Integer(<span class="dv">1</span>, <span class="dv">30</span>),  <span class="co"># Range for n_neighbors</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;classifier__weights&#39;</span>: Categorical([<span class="st">&#39;uniform&#39;</span>, <span class="st">&#39;distance&#39;</span>]),</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;classifier__p&#39;</span>: Integer(<span class="dv">1</span>, <span class="dv">2</span>)  <span class="co"># p=1 for Manhattan, p=2 for Euclidean distance</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Update the pipeline to use a KNN Classifier</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>pipeline_knn <span class="op">=</span> Pipeline(steps<span class="op">=</span>[(<span class="st">&#39;preprocessor&#39;</span>, preprocessor),</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>                               (<span class="st">&#39;classifier&#39;</span>, KNeighborsClassifier())])</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Configure BayesSearchCV for KNN</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>bayes_search_knn <span class="op">=</span> BayesSearchCV(</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>pipeline_knn,</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    search_spaces<span class="op">=</span>param_space_knn,</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    n_iter<span class="op">=</span><span class="dv">32</span>,  <span class="co"># Number of iterations</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span><span class="dv">5</span>,       <span class="co"># 5-fold cross-validation</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>,  <span class="co"># Use all available cores</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit BayesSearchCV to the data (assuming X_train, y_train are defined)</span></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>np.<span class="bu">int</span> <span class="op">=</span> <span class="bu">int</span>  <span class="co"># Temporary workaround for the np.int issue</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>bayes_search_knn.fit(X_train, y_train)</span></code></pre></div>
<h2 id="results-and-discussion-3">Results and Discussion</h2>
<p>After applying Bayesian optimization, the KNN model’s best parameters
were identified. The model achieved an accuracy of 0.8867 on the test
data, indicating its efficacy in classification tasks.</p>
<figure>
<img src="images/roc_curve_knn.png" id="fig:pr_knn"
alt="Precision-Recall Curve for KNN" />
<figcaption aria-hidden="true">Precision-Recall Curve for
KNN</figcaption>
</figure>
<figure>
<img src="images/pr_curve_knn.png" id="fig:pr_knn"
alt="Precision-Recall Curve for KNN" />
<figcaption aria-hidden="true">Precision-Recall Curve for
KNN</figcaption>
</figure>
<div id="tab:classification_report_knn">
<table>
<caption>Classification Report for KNN Model</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Class</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">F1-Score</th>
<th style="text-align: center;">Support</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">0 (No Heart Disease)</td>
<td style="text-align: center;">0.875000</td>
<td style="text-align: center;">0.887324</td>
<td style="text-align: center;">0.881119</td>
<td style="text-align: center;">71</td>
</tr>
<tr class="even">
<td style="text-align: left;">1 (With Heart Disease)</td>
<td style="text-align: center;">0.897436</td>
<td style="text-align: center;">0.886076</td>
<td style="text-align: center;">0.891720</td>
<td style="text-align: center;">79</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Accuracy</td>
<td colspan="4" style="text-align: center;">0.8866666666666667</td>
</tr>
<tr class="even">
<td style="text-align: left;">Macro Avg</td>
<td style="text-align: center;">0.886218</td>
<td style="text-align: center;">0.886700</td>
<td style="text-align: center;">0.886419</td>
<td style="text-align: center;">150</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Weighted Avg</td>
<td style="text-align: center;">0.886816</td>
<td style="text-align: center;">0.886667</td>
<td style="text-align: center;">0.886702</td>
<td style="text-align: center;">150</td>
</tr>
</tbody>
</table>
</div>
<p>The KNN classifier, with ROC AUC of 0.95 and PR AUC of 0.94,
demonstrates its capability in providing reliable classifications,
particularly beneficial for applications requiring interpretable and
straightforward decision-making processes.</p>
<h1 id="neural-network-model">Neural Network Model</h1>
<h2 id="model-explanation">Model Explanation</h2>
<p>Neural Networks are a subset of machine learning algorithms modeled
after the human brain. They consist of layers of interconnected nodes or
’neurons’, each capable of performing simple computations. These
networks are highly effective for complex tasks like pattern recognition
and classification.</p>
<h2 id="implementation-4">Implementation</h2>
<p>The implementation of the Neural Network model in this project
involves hyperparameter tuning using Keras Tuner. This process includes
defining a range of values for parameters like the number of units in
each layer, dropout rate, and learning rate. Keras Tuner then
iteratively searches for the optimal combination of these
hyperparameters.</p>
<div class="sourceCode" id="cb14" data-language="Python"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> Dense, Dropout</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.optimizers <span class="im">import</span> Adam</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> kerastuner <span class="im">as</span> kt</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to build the model</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_model(hp):</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... [Model building code] ...</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize and configure the tuner</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>tuner <span class="op">=</span> kt.Hyperband(build_model, ...)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform the hyperparameter search</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>tuner.search(X_train_transformed, y_train, epochs<span class="op">=</span><span class="dv">10</span>, validation_split<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Retrieve and print the best hyperparameters</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>best_hps <span class="op">=</span> tuner.get_best_hyperparameters(num_trials<span class="op">=</span><span class="dv">1</span>)[<span class="dv">0</span>]</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;The optimal number of units in the first layer is ...&quot;</span>)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Build and train the model</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> tuner.hypermodel.build(best_hps)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train_transformed, y_train, epochs<span class="op">=</span><span class="dv">10</span>, validation_split<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>test_loss, test_accuracy <span class="op">=</span> model.evaluate(X_test_transformed, y_test)</span></code></pre></div>
<h2 id="results-and-discussion-4">Results and Discussion</h2>
<p>The optimized Neural Network model showed promising results,
achieving an accuracy of 81.33% on the test data. The hyperparameter
tuning process identified the optimal number of neurons and learning
rate, leading to a well-performing model.</p>
<figure>
<img src="images/training_history.png" id="fig:training_history_nn"
style="width:80.0%" alt="Training History of the Neural Network" />
<figcaption aria-hidden="true">Training History of the Neural
Network</figcaption>
</figure>
<p>The model’s performance can be further analyzed and improved by
experimenting with different architectures, activation functions, and
optimization strategies. Neural Networks’ ability to model complex
relationships in data makes them a powerful tool for various predictive
tasks.</p>
<h1 id="sec:conclusion">Conclusion and Final Discussion</h1>
<p>This study aimed to explore various machine learning models for the
prediction of heart disease. Through rigorous data preprocessing,
implementation, and optimization of several models, we have gained
valuable insights into the performance and applicability of each model
in the context of heart disease prediction.</p>
<h2 id="comparative-analysis-of-models">Comparative Analysis of
Models</h2>
<p>A comparative analysis was conducted to evaluate the performance of
Logistic Regression, Support Vector Machine (SVM), Decision Tree, Random
Forest, K-Nearest Neighbors (KNN), and Neural Network models. The key
performance metrics considered were accuracy, precision, recall,
F1-score, ROC-AUC, and Precision-Recall AUC.</p>
<h3 id="model-evaluation-curves">Model Evaluation Curves</h3>
<p>In evaluating the performance of the predictive models, the Receiver
Operating Characteristic (ROC) and Precision-Recall (PR) curves provide
insightful metrics into the models’ true positive rate and
precision-recall balance, respectively. The ROC curves of all models are
plotted in Figure <a href="#fig:roc_curves" data-reference-type="ref"
data-reference="fig:roc_curves">11</a>, illustrating their respective
Area Under the Curve (AUC) scores.</p>
<figure>
<img src="images/roc_curve_combined.png" id="fig:roc_curves"
style="width:70.0%" alt="Combined ROC Curves for all models" />
<figcaption aria-hidden="true">Combined ROC Curves for all
models</figcaption>
</figure>
<figure>
<img src="images/pr_curve_combined.png" id="fig:pr_curves" style="width:70.0%"
alt="Combined Precision-Recall Curves for all models" />
<figcaption aria-hidden="true">Combined Precision-Recall Curves for all
models</figcaption>
</figure>
<p>From the curves, we can observe that the Random Forest model not only
has a higher AUC for the ROC curve but also maintains a superior balance
in the Precision-Recall curve. This suggests that the Random Forest
model is not only good at distinguishing between the positive and
negative classes but also maintains a high precision even as recall
increases, which is ideal for medical diagnostic purposes where the cost
of false negatives is high.</p>
<h3 id="results-table">Results Table</h3>
<div id="tab:model_comparison">
<table>
<caption>Comparative Performance Metrics of Different Models</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">F1-Score</th>
<th style="text-align: center;">ROC-AUC</th>
<th style="text-align: center;">PR-AUC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Logistic Regression</td>
<td style="text-align: center;">89.33%</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.96</td>
</tr>
<tr class="even">
<td style="text-align: left;">SVM</td>
<td style="text-align: center;">89.33%</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.96</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Decision Tree</td>
<td style="text-align: center;">82.67%</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">0.92</td>
</tr>
<tr class="even">
<td style="text-align: left;">Random Forest</td>
<td style="text-align: center;">92.00%</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.95</td>
</tr>
<tr class="odd">
<td style="text-align: left;">KNN</td>
<td style="text-align: center;">88.67%</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.94</td>
</tr>
<tr class="even">
<td style="text-align: left;">Neural Network</td>
<td style="text-align: center;">81.33%</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
</div>
<h2 id="discussion">Discussion</h2>
<p>The results indicate that the Random Forest model outperformed other
models in terms of accuracy, precision, recall, and F1-score. This
model’s ensemble nature, which combines multiple decision trees, likely
contributed to its superior ability to capture complex patterns in the
data, leading to better generalization and robustness against
overfitting.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In conclusion, this study demonstrates the efficacy of machine
learning in predicting heart disease. The Random Forest model, in
particular, shows great promise for clinical application. However, each
model presents unique strengths and limitations, suggesting that a
multi-model approach might be beneficial in real-world settings to
provide a comprehensive analysis for heart disease prediction.</p>
</body>
</html>
